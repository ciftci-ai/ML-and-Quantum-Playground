{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0N_ItXyc1vC"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "- **Chatbot with Transformer Models**:  \n",
        "A chatbot is a conversational agent that interacts with users in natural language. Using pre-trained transformer models like GPT (Generative Pre-trained Transformers), chatbots can understand and generate human-like responses. The goal is to create a chatbot that can hold coherent and contextually relevant conversations with users over multiple turns.\n",
        "\n",
        "- **Language Modeling and Text Generation**:  \n",
        "Language modeling is the task of predicting the next word or sequence of words in a sentence based on the context provided by previous words. Using pre-trained GPT (Generative Pre-trained Transformer) models, we can train a model to generate creative text, such as poetry or short stories, and build a text autocomplete system. The goal is to create a model that understands the structure and style of the input text and can generate coherent, contextually appropriate continuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZCLGWtIHVa"
      },
      "source": [
        "## Chatbot with Transformer Models\n",
        "\n",
        "### Problem Description:\n",
        "A chatbot is a conversational agent that interacts with users in natural language. Using pre-trained transformer models like GPT (Generative Pre-trained Transformers), chatbots can understand and generate human-like responses. The goal is to create a chatbot that can hold coherent and contextually relevant conversations with users over multiple turns.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Transformer Models**: Deep learning models designed to understand the context and relationships between words in a sequence. They excel in tasks like language understanding and generation.\n",
        "2. **Tokenization**: The process of breaking down text into smaller units (tokens) that the model can understand.\n",
        "3. **Context**: The previous conversation history that helps the model generate responses relevant to the current input.\n",
        "4. **Inference**: The process of generating a response based on user input using a pre-trained model.\n",
        "5. **Pre-Trained Model**: A model that has been trained on a large corpus of text data and can be fine-tuned for specific tasks (e.g., GPT-2, DialoGPT).\n",
        "\n",
        "### Chatbot Process:\n",
        "1. **Input Processing**: The user inputs a message, which is tokenized for the model to understand.\n",
        "2. **Generate Response**: The model uses the input and conversation history to generate a relevant response.\n",
        "3. **Output Decoding**: The generated tokens are decoded back into human-readable text.\n",
        "4. **Conversation Flow**: The chat history is updated with each interaction, allowing the model to maintain context across multiple turns.\n",
        "\n",
        "\n",
        "### Steps:\n",
        "1. **Select a Pre-Trained Model**: Choose a model like GPT-2 or DialoGPT that is suitable for conversational tasks.\n",
        "2. **Tokenize Input**: Process user input into tokens that the model can understand.\n",
        "3. **Generate Responses**: Use the model to generate a response based on the input and conversation history.\n",
        "4. **Decode Output**: Convert the model's output tokens back into human-readable text.\n",
        "5. **Conversation Management**: Update the conversation history with each user and chatbot interaction to maintain context.\n",
        "6. **Fine-Tuning**: Optionally, fine-tune the model on domain-specific conversations to improve relevance and accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JynNV2bvGyJz",
        "outputId": "2da44522-ea43-472f-a556-cb5dbbb5d027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot is ready! Type 'quit' to end the conversation.\n",
            "You: Hi\n",
            "Chatbot: Hi!\n",
            "You: What is the weather like today?\n",
            "Chatbot: It's getting colder. It's been freezing for the last few days.\n",
            "You: Tell me a joke.\n",
            "Chatbot: Did you hear about Pluto?\n",
            "You: No\n",
            "Chatbot: It wasn't a joke.\n",
            "You: quit\n",
            "Ending the chat. Have a great day!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained DialoGPT model and tokenizer\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "def generate_response(input_text, chat_history_ids=None):\n",
        "    # Encode the user input and add end-of-string token\n",
        "    new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Append the new user input to the chat history (if any)\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
        "\n",
        "    # Generate the response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode the response and print it\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response, chat_history_ids\n",
        "\n",
        "# Initialize the chat history\n",
        "chat_history_ids = None\n",
        "print(\"Chatbot is ready! Type 'quit' to end the conversation.\")\n",
        "\n",
        "while True:\n",
        "    # Get user input\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Ending the chat. Have a great day!\")\n",
        "        break\n",
        "\n",
        "    # Generate response\n",
        "    response, chat_history_ids = generate_response(user_input, chat_history_ids)\n",
        "\n",
        "    print(f\"Chatbot: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaIYBdHIp2M"
      },
      "source": [
        "## Language Modeling and Text Generation\n",
        "\n",
        "### Problem Description:\n",
        "Language modeling is the task of predicting the next word or sequence of words in a sentence based on the context provided by previous words. Using pre-trained GPT (Generative Pre-trained Transformer) models, we can train a model to generate creative text, such as poetry or short stories, and build a text autocomplete system. The goal is to create a model that understands the structure and style of the input text and can generate coherent, contextually appropriate continuations.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Language Model**: A model that learns the probability of a word or sequence of words occurring based on the context provided by preceding words.\n",
        "2. **Pre-Trained Model**: A model like GPT-2 that has been trained on a large corpus of text data and can be fine-tuned for specific text generation tasks.\n",
        "3. **Tokenization**: The process of converting text into tokens that the model can understand, such as words or subwords.\n",
        "4. **Fine-Tuning**: The process of further training a pre-trained model on a specific dataset to adapt it to a particular style or domain.\n",
        "5. **Creative Writing**: Using the model to generate poetry, short stories, or other forms of creative text based on a given prompt.\n",
        "6. **Text Autocomplete**: Predicting and suggesting completions for partial text inputs, similar to how search engines provide suggestions as users type.\n",
        "\n",
        "### Language Modeling Process:\n",
        "1. **Data Collection**: Gather a dataset of creative writing samples (e.g., poetry, short stories) for fine-tuning.\n",
        "2. **Tokenize Text**: Process the text into tokens using a tokenizer compatible with the chosen GPT model.\n",
        "3. **Fine-Tune the Model**: Train the pre-trained GPT model on the dataset to capture the style and structure of creative writing.\n",
        "4. **Generate Text**: Provide a prompt to the model and generate creative continuations.\n",
        "5. **Autocomplete System**: Use the model to suggest likely continuations for user inputs in real-time.\n",
        "\n",
        "### Types of Language Models:\n",
        "1. **Generative Language Models**: Create new text sequences based on input prompts (e.g., GPT-2, GPT-3).\n",
        "2. **Masked Language Models**: Predict missing words in a sentence (e.g., BERT).\n",
        "3. **Seq2Seq Models**: Translate or transform one sequence into another (e.g., for translation or summarization tasks).\n",
        "\n",
        "### Language Modeling Objective:\n",
        "The goal is to train a GPT-based model to:\n",
        "- Generate creative text that matches the style and theme of the input.\n",
        "- Autocomplete partial sentences by suggesting relevant continuations.\n",
        "- Adapt to different creative tasks such as poetry generation or story continuation.\n",
        "\n",
        "### Steps:\n",
        "1. **Select a Pre-Trained Model**: Choose a model like GPT-2 from the Hugging Face Transformers library.\n",
        "2. **Prepare Data**: Tokenize the dataset of creative writing samples.\n",
        "3. **Fine-Tune the Model**: Train the model using the prepared dataset to adapt it to the desired style.\n",
        "4. **Text Generation**: Use the fine-tuned model to generate text based on user prompts.\n",
        "5. **Build Autocomplete**: Use the model to predict likely word sequences based on partially typed text.\n",
        "6. **Interactive Example**: Create a loop where users can input a prompt, and the model generates creative continuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPf-MD7tIvZ9",
        "outputId": "49bccd71-5831-4857-fa34-243d64315e37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt:\n",
            " The sun dipped below the horizon, casting a golden glow over the sea\n",
            "Generated Poem:\n",
            " The sun dipped below the horizon, casting a golden glow over the sea of green. The rain fell from the sky, and the clouds floated over it.\n",
            "\n",
            "The moon rose in a flash, the sun's rays hitting the moon like a jet and leaving a cloud of red in its wake. Then the rays of the wind swept through the air, illuminating the night sky with its shimmering light. An incredible amount of light shone through its blackened surface, creating a perfect storm of sparks and\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "def generate_text(prompt, max_length=100, temperature=0.7):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text using the GPT-2 model with sampling\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature,  # Controls randomness in generation\n",
        "        top_k=50,                 # Filters the most likely words\n",
        "        top_p=0.95,               # Nucleus sampling\n",
        "        do_sample=True,           # Enables sampling for more creative results\n",
        "        no_repeat_ngram_size=2,   # Prevents repeating phrases\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the output to readable text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "    return generated_text\n",
        "\n",
        "# Example prompt for generating a poem\n",
        "prompt = \"The sun dipped below the horizon, casting a golden glow over the sea\"\n",
        "generated_poem = generate_text(prompt, max_length=100)\n",
        "print(\"Prompt:\\n\", prompt)\n",
        "print(\"Generated Poem:\\n\", generated_poem)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Kk2Zdh0JkTb",
        "outputId": "c0455ea9-cd11-48b3-9f98-1d71ae326394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: In the quiet forest, a fox\n",
            "Autocomplete Suggestion: In the quiet forest, a fox is a fox, and a wolf is a wolf, and a bear is a bear.\n"
          ]
        }
      ],
      "source": [
        "def autocomplete_text(prompt, max_length=50, temperature=0.7):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text that completes the prompt using sampling\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=temperature,  # Controls randomness (higher values = more diverse outputs)\n",
        "        top_k=50,                 # Filters to the top 50 most likely next tokens\n",
        "        top_p=0.9,                # Uses nucleus sampling (picking from the top 90% probability mass)\n",
        "        do_sample=True,           # Enables sampling for more creative output\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return the completion\n",
        "    completion = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return completion\n",
        "\n",
        "# Example prompt for autocomplete\n",
        "prompt = \"In the quiet forest, a fox\"\n",
        "completion = autocomplete_text(prompt, max_length=50)\n",
        "print(\"Prompt:\", prompt)\n",
        "print(\"Autocomplete Suggestion:\\n\", completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yu3dXX8JttP",
        "outputId": "cb3f339d-1c68-4e6f-e88c-469c2e5c30ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Text Generation Demo! Type 'quit' to exit.\n",
            "Enter a prompt: The ancient oak tree stood tall in the clearing, its branches\n",
            "\n",
            "Generated Text:\n",
            " The ancient oak tree stood tall in the clearing, its branches hanging from its trunk.\n",
            "\n",
            "\"What's the matter with this?\"\n",
            "...\n",
            "-\n",
            "There was a long silence, and then, as if the sky were changing, the sound of the wind rose to a loud and sudden roar. As if by magic, a large black blade pierced through the oak's branches, piercing the tree with its long blade. The blade struck the branch, cutting it in half and killing it\n",
            "Enter a prompt: quit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "print(\"Welcome to the Text Generation Demo! Type 'quit' to exit.\")\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Enter a prompt: \")\n",
        "    if user_prompt.lower() == 'quit':\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Generate creative continuation\n",
        "    generated_text = generate_text(user_prompt, max_length=100)\n",
        "    print(\"\\nGenerated Text:\\n\", generated_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "U5irvGkoQitG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
